{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (5000, 10)\n",
      "\n",
      "Missing Values:\n",
      " Temperature                      0\n",
      "Humidity                         0\n",
      "PM2.5                            0\n",
      "PM10                             0\n",
      "NO2                              0\n",
      "SO2                              0\n",
      "CO                               0\n",
      "Proximity_to_Industrial_Areas    0\n",
      "Population_Density               0\n",
      "Air Quality                      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('updated_pollution_dataset.csv')\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nMissing Values:\\n\", df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values if any\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['Temperature', 'Humidity', 'PM2.5', 'PM10', 'NO2', 'SO2', 'CO', 'Proximity_to_Industrial_Areas', 'Population_Density', 'Air Quality']\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('updated_pollution_dataset.csv')\n",
    "print(\"Columns:\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense_layer:\n",
    "    \"\"\"\n",
    "    This class is used to define the dense layer in Neural Networks. \n",
    "    This included forward and backward propagation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        \"\"\"\n",
    "        n_inputs: Number of inputs.\n",
    "        n_neurons: Number of neurons in the layer\n",
    "\n",
    "        Weights are defined with random values.\n",
    "        Biases are defined as zeros.\n",
    "        \"\"\"\n",
    "        self.weights = np.random.rand(n_inputs, n_neurons)  # randomly initialized weights\n",
    "        self.biases = np.zeros((1, n_neurons)) # biases intialized as zeros\n",
    "\n",
    "    def forward_propagation(self, input_layer):\n",
    "        self.input_layer = input_layer\n",
    "        self.output_layer = np.dot(input_layer, self.weights) + self.biases\n",
    "        return self.output_layer\n",
    "    \n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        self.d_weights = np.dot(self.input_layer.T, output_error) / self.input_layer.shape[0]\n",
    "        self.d_biases = np.sum(output_error, axis=0, keepdims=True) / self.input_layer.shape[0]\n",
    "        self.d_inputs = np.dot(output_error, self.weights.T)\n",
    "\n",
    "        # Update weights and biases\n",
    "        self.weights -= learning_rate * self.d_weights\n",
    "        self.biases -= learning_rate * self.d_biases\n",
    "\n",
    "        return self.d_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid - Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    \"\"\" \n",
    "    This class represents the sigmoid activation function.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward_propagation(self, input):\n",
    "        self.inputs = input\n",
    "        self.output = 1 / (1 + np.exp(-input))\n",
    "        return self.output\n",
    "    \n",
    "    def backward_propagation(self, output_error):\n",
    "        return output_error * (self.output * (1 - self.output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relu Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    \"\"\" \n",
    "    This class is for define Rectified Linear Unit (Relu) activation funciton.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward_propagation(self, input):\n",
    "        self.input = input\n",
    "        self.output = np.maximum(0, input)\n",
    "        return self.output\n",
    "    \n",
    "    def backward_propagation(self, output_error):\n",
    "        return output_error * (self.output > 0).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax - Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    \"\"\" \n",
    "    This class is for define Softmax activation funciton.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward_propagation(self, input):\n",
    "        self.input = input\n",
    "        ex = np.exp(input - np.max(input, axis=1, keepdims=True))\n",
    "        self.output = ex / np.sum(ex, axis=1, keepdims=True)\n",
    "        return self.output\n",
    "    \n",
    "    def backward_propagation(self, output_error):\n",
    "        return output_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__(self, dropout_probabality):\n",
    "        self.dropout_probabality = dropout_probabality\n",
    "        self.mask = None\n",
    "\n",
    "    def forward_propagation(self, input):\n",
    "        if self.dropout_probabality < 1.0:\n",
    "            self.mask = (np.random.rand(*input.shape) >\n",
    "                         self.dropout_probabality) / (1 - self.dropout_probabality)\n",
    "\n",
    "            return input * self.mask\n",
    "        return input\n",
    "\n",
    "    def backward_propagation(self, output_error):\n",
    "        if self.dropout_probabality < 1.0:\n",
    "            return output_error * self.mask\n",
    "        return output_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_network:\n",
    "   # Modified initialization to support optimizers\n",
    "   def __init__(self, optimizer_type='sgd_momentum', **optimizer_params):\n",
    "       self.layers = []\n",
    "       #New: optimizer selection based on type\n",
    "       if optimizer_type == 'sgd_momentum':\n",
    "           self.optimizer = SGD_Momentum(**optimizer_params)\n",
    "       else:\n",
    "           self.optimizer = MiniBatchSGD(**optimizer_params)\n",
    "\n",
    "   def add_layer(self, layer, activation_func=None, dropout=None):\n",
    "       self.layers.append(\n",
    "           {\"layer\": layer, \"activation_func\": activation_func, \"dropout\": dropout})\n",
    "    #New: method to collect parameters for optimizer \n",
    "   \n",
    "   def get_parameters(self):\n",
    "        params = []\n",
    "        for layer_details in self.layers:\n",
    "            if hasattr(layer_details[\"layer\"], \"weights\"):\n",
    "                params.append({\n",
    "                    'weights': layer_details[\"layer\"].weights,\n",
    "                    'biases': layer_details[\"layer\"].biases\n",
    "                })\n",
    "        return params\n",
    "        \n",
    "   \n",
    "    #New : method to collect gradients \n",
    "    \n",
    "    \n",
    "   def get_gradients(self):\n",
    "        gradients = []\n",
    "        for layer_details in self.layers:\n",
    "            if hasattr(layer_details[\"layer\"], \"d_weights\"):\n",
    "                gradients.append({\n",
    "                    'd_weights': layer_details[\"layer\"].d_weights,\n",
    "                    'd_biases': layer_details[\"layer\"].d_biases\n",
    "                })\n",
    "        return gradients\n",
    "\n",
    "\n",
    "   def forward_propagation(self, X):\n",
    "       self.input = X\n",
    "       for layer_details in self.layers:\n",
    "           X = layer_details[\"layer\"].forward_propagation(X)\n",
    "           if layer_details[\"activation_func\"] is not None:\n",
    "               X = layer_details[\"activation_func\"].forward_propagation(X)\n",
    "           if layer_details[\"dropout\"] is not None:\n",
    "               X = layer_details[\"dropout\"].forward_propagation(X)\n",
    "       self.output = X\n",
    "       return self.output\n",
    "\n",
    "   def backward_propagation(self, output_error, learning_rate):\n",
    "       for layer_details in reversed(self.layers):\n",
    "            if layer_details[\"dropout\"]:\n",
    "                output_error = layer_details[\"dropout\"].backward_propagation(\n",
    "                    output_error)\n",
    "            if layer_details[\"activation_func\"]:\n",
    "                output_error = layer_details[\"activation_func\"].backward_propagation(\n",
    "                    output_error)\n",
    "            output_error = layer_details[\"layer\"].backward_propagation(\n",
    "               output_error, learning_rate)\n",
    "\n",
    "   # NEW: optimizer update step\n",
    "\n",
    "       params = self.get_parameters()\n",
    "       gradients = self.get_gradients()\n",
    "       self.optimizer.update(params, gradients)\n",
    "\n",
    "# Modified training method to work with optimizer\n",
    "   def train(self, X, y,epochs, learning_rate=0.01, batch_size=32):\n",
    "        # NEW: Initialize optimizer parameters\n",
    "        self.optimizer.initialize(self.get_parameters())\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            for i in range(0, X.shape[0], batch_size):\n",
    "                X_batch = X[i:i+batch_size]\n",
    "                y_batch = y[i:i+batch_size]\n",
    "                predictions = self.forward_propagation(X_batch)\n",
    "                #categorical cross-entropy loss\n",
    "                loss = - \\\n",
    "                    np.mean(np.sum(y_batch * np.log(predictions + 1e-7), axis=1))\n",
    "                output = predictions - y_batch\n",
    "                self.backward_propagation(output, learning_rate)\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}\")\n",
    "\n",
    "   def predict(self, X):\n",
    "        prediction = self.forward_propagation(X)\n",
    "        return np.argmax(prediction, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing, Balancing, and Scaling for Air Quality Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the dataset and feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"updated_pollution_dataset.csv\")\n",
    "\n",
    "X = df.drop(\"Air Quality\", axis=1).values\n",
    "y = df[\"Air Quality\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding of the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder()\n",
    "y_one_hot = encoder.fit_transform(y.reshape(-1, 1)).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Balancing the dataset using SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance classes using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_balanced, y_balanced = smote.fit_resample(X, np.argmax(y_one_hot, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert y_balanced back to numeric values (already integers after SMOTE)\n",
    "y_balanced = np.array(y_balanced).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the balanced target\n",
    "y_balanced = encoder.fit_transform(y_balanced).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Split and Scaling features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_balanced, y_balanced, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Neural Network Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nn = Neural_network()\n",
    "nn.add_layer(Dense_layer(X_train.shape[1], 16), Relu(),Dropout(0.05))\n",
    "nn.add_layer(Dense_layer(16, 8), Relu(),Dropout(0.05))\n",
    "nn.add_layer(Dense_layer(8, 4), Softmax())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300, Loss: 0.7853\n",
      "Epoch 2/300, Loss: 0.6059\n",
      "Epoch 3/300, Loss: 0.5745\n",
      "Epoch 4/300, Loss: 0.5463\n",
      "Epoch 5/300, Loss: 0.5353\n",
      "Epoch 6/300, Loss: 0.4293\n",
      "Epoch 7/300, Loss: 0.3457\n",
      "Epoch 8/300, Loss: 0.2862\n",
      "Epoch 9/300, Loss: 0.3358\n",
      "Epoch 10/300, Loss: 0.2953\n",
      "Epoch 11/300, Loss: 0.3359\n",
      "Epoch 12/300, Loss: 0.3696\n",
      "Epoch 13/300, Loss: 0.2433\n",
      "Epoch 14/300, Loss: 0.2481\n",
      "Epoch 15/300, Loss: 0.2633\n",
      "Epoch 16/300, Loss: 0.3310\n",
      "Epoch 17/300, Loss: 0.2557\n",
      "Epoch 18/300, Loss: 0.2470\n",
      "Epoch 19/300, Loss: 0.2416\n",
      "Epoch 20/300, Loss: 0.1719\n",
      "Epoch 21/300, Loss: 0.2239\n",
      "Epoch 22/300, Loss: 0.2051\n",
      "Epoch 23/300, Loss: 0.1903\n",
      "Epoch 24/300, Loss: 0.2297\n",
      "Epoch 25/300, Loss: 0.2052\n",
      "Epoch 26/300, Loss: 0.2164\n",
      "Epoch 27/300, Loss: 0.2314\n",
      "Epoch 28/300, Loss: 0.2154\n",
      "Epoch 29/300, Loss: 0.2721\n",
      "Epoch 30/300, Loss: 0.1683\n",
      "Epoch 31/300, Loss: 0.1629\n",
      "Epoch 32/300, Loss: 0.1898\n",
      "Epoch 33/300, Loss: 0.1542\n",
      "Epoch 34/300, Loss: 0.1972\n",
      "Epoch 35/300, Loss: 0.1745\n",
      "Epoch 36/300, Loss: 0.2120\n",
      "Epoch 37/300, Loss: 0.1852\n",
      "Epoch 38/300, Loss: 0.2210\n",
      "Epoch 39/300, Loss: 0.1564\n",
      "Epoch 40/300, Loss: 0.1434\n",
      "Epoch 41/300, Loss: 0.1859\n",
      "Epoch 42/300, Loss: 0.1581\n",
      "Epoch 43/300, Loss: 0.1806\n",
      "Epoch 44/300, Loss: 0.1607\n",
      "Epoch 45/300, Loss: 0.1683\n",
      "Epoch 46/300, Loss: 0.1857\n",
      "Epoch 47/300, Loss: 0.1721\n",
      "Epoch 48/300, Loss: 0.1907\n",
      "Epoch 49/300, Loss: 0.1837\n",
      "Epoch 50/300, Loss: 0.1650\n",
      "Epoch 51/300, Loss: 0.1738\n",
      "Epoch 52/300, Loss: 0.2028\n",
      "Epoch 53/300, Loss: 0.1734\n",
      "Epoch 54/300, Loss: 0.1892\n",
      "Epoch 55/300, Loss: 0.2280\n",
      "Epoch 56/300, Loss: 0.2640\n",
      "Epoch 57/300, Loss: 0.1488\n",
      "Epoch 58/300, Loss: 0.1617\n",
      "Epoch 59/300, Loss: 0.1913\n",
      "Epoch 60/300, Loss: 0.1858\n",
      "Epoch 61/300, Loss: 0.1697\n",
      "Epoch 62/300, Loss: 0.1970\n",
      "Epoch 63/300, Loss: 0.2511\n",
      "Epoch 64/300, Loss: 0.1852\n",
      "Epoch 65/300, Loss: 0.1865\n",
      "Epoch 66/300, Loss: 0.1317\n",
      "Epoch 67/300, Loss: 0.2121\n",
      "Epoch 68/300, Loss: 0.2184\n",
      "Epoch 69/300, Loss: 0.1898\n",
      "Epoch 70/300, Loss: 0.2425\n",
      "Epoch 71/300, Loss: 0.1825\n",
      "Epoch 72/300, Loss: 0.1491\n",
      "Epoch 73/300, Loss: 0.1893\n",
      "Epoch 74/300, Loss: 0.1978\n",
      "Epoch 75/300, Loss: 0.2390\n",
      "Epoch 76/300, Loss: 0.1637\n",
      "Epoch 77/300, Loss: 0.1894\n",
      "Epoch 78/300, Loss: 0.1498\n",
      "Epoch 79/300, Loss: 0.1724\n",
      "Epoch 80/300, Loss: 0.1825\n",
      "Epoch 81/300, Loss: 0.1728\n",
      "Epoch 82/300, Loss: 0.1781\n",
      "Epoch 83/300, Loss: 0.2059\n",
      "Epoch 84/300, Loss: 0.1657\n",
      "Epoch 85/300, Loss: 0.1882\n",
      "Epoch 86/300, Loss: 0.1214\n",
      "Epoch 87/300, Loss: 0.1934\n",
      "Epoch 88/300, Loss: 0.1750\n",
      "Epoch 89/300, Loss: 0.1608\n",
      "Epoch 90/300, Loss: 0.1999\n",
      "Epoch 91/300, Loss: 0.1460\n",
      "Epoch 92/300, Loss: 0.1535\n",
      "Epoch 93/300, Loss: 0.1866\n",
      "Epoch 94/300, Loss: 0.1792\n",
      "Epoch 95/300, Loss: 0.2057\n",
      "Epoch 96/300, Loss: 0.1311\n",
      "Epoch 97/300, Loss: 0.1465\n",
      "Epoch 98/300, Loss: 0.1890\n",
      "Epoch 99/300, Loss: 0.1593\n",
      "Epoch 100/300, Loss: 0.1515\n",
      "Epoch 101/300, Loss: 0.1523\n",
      "Epoch 102/300, Loss: 0.1668\n",
      "Epoch 103/300, Loss: 0.1748\n",
      "Epoch 104/300, Loss: 0.1703\n",
      "Epoch 105/300, Loss: 0.1998\n",
      "Epoch 106/300, Loss: 0.1920\n",
      "Epoch 107/300, Loss: 0.1964\n",
      "Epoch 108/300, Loss: 0.2394\n",
      "Epoch 109/300, Loss: 0.1732\n",
      "Epoch 110/300, Loss: 0.1707\n",
      "Epoch 111/300, Loss: 0.1770\n",
      "Epoch 112/300, Loss: 0.1747\n",
      "Epoch 113/300, Loss: 0.1724\n",
      "Epoch 114/300, Loss: 0.1886\n",
      "Epoch 115/300, Loss: 0.1603\n",
      "Epoch 116/300, Loss: 0.1648\n",
      "Epoch 117/300, Loss: 0.1845\n",
      "Epoch 118/300, Loss: 0.1706\n",
      "Epoch 119/300, Loss: 0.2071\n",
      "Epoch 120/300, Loss: 0.1660\n",
      "Epoch 121/300, Loss: 0.1049\n",
      "Epoch 122/300, Loss: 0.1637\n",
      "Epoch 123/300, Loss: 0.1610\n",
      "Epoch 124/300, Loss: 0.1798\n",
      "Epoch 125/300, Loss: 0.1571\n",
      "Epoch 126/300, Loss: 0.1354\n",
      "Epoch 127/300, Loss: 0.2155\n",
      "Epoch 128/300, Loss: 0.1733\n",
      "Epoch 129/300, Loss: 0.1736\n",
      "Epoch 130/300, Loss: 0.1953\n",
      "Epoch 131/300, Loss: 0.1529\n",
      "Epoch 132/300, Loss: 0.1571\n",
      "Epoch 133/300, Loss: 0.1843\n",
      "Epoch 134/300, Loss: 0.1626\n",
      "Epoch 135/300, Loss: 0.1608\n",
      "Epoch 136/300, Loss: 0.1694\n",
      "Epoch 137/300, Loss: 0.2076\n",
      "Epoch 138/300, Loss: 0.1615\n",
      "Epoch 139/300, Loss: 0.1371\n",
      "Epoch 140/300, Loss: 0.1593\n",
      "Epoch 141/300, Loss: 0.1531\n",
      "Epoch 142/300, Loss: 0.1490\n",
      "Epoch 143/300, Loss: 0.1763\n",
      "Epoch 144/300, Loss: 0.1641\n",
      "Epoch 145/300, Loss: 0.1575\n",
      "Epoch 146/300, Loss: 0.1973\n",
      "Epoch 147/300, Loss: 0.2025\n",
      "Epoch 148/300, Loss: 0.1547\n",
      "Epoch 149/300, Loss: 0.1832\n",
      "Epoch 150/300, Loss: 0.1539\n",
      "Epoch 151/300, Loss: 0.1814\n",
      "Epoch 152/300, Loss: 0.1680\n",
      "Epoch 153/300, Loss: 0.1776\n",
      "Epoch 154/300, Loss: 0.1481\n",
      "Epoch 155/300, Loss: 0.1709\n",
      "Epoch 156/300, Loss: 0.1782\n",
      "Epoch 157/300, Loss: 0.1781\n",
      "Epoch 158/300, Loss: 0.1546\n",
      "Epoch 159/300, Loss: 0.1368\n",
      "Epoch 160/300, Loss: 0.1621\n",
      "Epoch 161/300, Loss: 0.1553\n",
      "Epoch 162/300, Loss: 0.1753\n",
      "Epoch 163/300, Loss: 0.1788\n",
      "Epoch 164/300, Loss: 0.1332\n",
      "Epoch 165/300, Loss: 0.1661\n",
      "Epoch 166/300, Loss: 0.1413\n",
      "Epoch 167/300, Loss: 0.1734\n",
      "Epoch 168/300, Loss: 0.1705\n",
      "Epoch 169/300, Loss: 0.2149\n",
      "Epoch 170/300, Loss: 0.1660\n",
      "Epoch 171/300, Loss: 0.1737\n",
      "Epoch 172/300, Loss: 0.1602\n",
      "Epoch 173/300, Loss: 0.2138\n",
      "Epoch 174/300, Loss: 0.1535\n",
      "Epoch 175/300, Loss: 0.1776\n",
      "Epoch 176/300, Loss: 0.1402\n",
      "Epoch 177/300, Loss: 0.1287\n",
      "Epoch 178/300, Loss: 0.1645\n",
      "Epoch 179/300, Loss: 0.1342\n",
      "Epoch 180/300, Loss: 0.1595\n",
      "Epoch 181/300, Loss: 0.1746\n",
      "Epoch 182/300, Loss: 0.1463\n",
      "Epoch 183/300, Loss: 0.1516\n",
      "Epoch 184/300, Loss: 0.1408\n",
      "Epoch 185/300, Loss: 0.1576\n",
      "Epoch 186/300, Loss: 0.1459\n",
      "Epoch 187/300, Loss: 0.1644\n",
      "Epoch 188/300, Loss: 0.1499\n",
      "Epoch 189/300, Loss: 0.1461\n",
      "Epoch 190/300, Loss: 0.1563\n",
      "Epoch 191/300, Loss: 0.1381\n",
      "Epoch 192/300, Loss: 0.1340\n",
      "Epoch 193/300, Loss: 0.1551\n",
      "Epoch 194/300, Loss: 0.1499\n",
      "Epoch 195/300, Loss: 0.1647\n",
      "Epoch 196/300, Loss: 0.1247\n",
      "Epoch 197/300, Loss: 0.2352\n",
      "Epoch 198/300, Loss: 0.1624\n",
      "Epoch 199/300, Loss: 0.1344\n",
      "Epoch 200/300, Loss: 0.1704\n",
      "Epoch 201/300, Loss: 0.1436\n",
      "Epoch 202/300, Loss: 0.1804\n",
      "Epoch 203/300, Loss: 0.1510\n",
      "Epoch 204/300, Loss: 0.1430\n",
      "Epoch 205/300, Loss: 0.1428\n",
      "Epoch 206/300, Loss: 0.2019\n",
      "Epoch 207/300, Loss: 0.1500\n",
      "Epoch 208/300, Loss: 0.1788\n",
      "Epoch 209/300, Loss: 0.1971\n",
      "Epoch 210/300, Loss: 0.1784\n",
      "Epoch 211/300, Loss: 0.1474\n",
      "Epoch 212/300, Loss: 0.1390\n",
      "Epoch 213/300, Loss: 0.1178\n",
      "Epoch 214/300, Loss: 0.1639\n",
      "Epoch 215/300, Loss: 0.2461\n",
      "Epoch 216/300, Loss: 0.2065\n",
      "Epoch 217/300, Loss: 0.1406\n",
      "Epoch 218/300, Loss: 0.2396\n",
      "Epoch 219/300, Loss: 0.1935\n",
      "Epoch 220/300, Loss: 0.1745\n",
      "Epoch 221/300, Loss: 0.1646\n",
      "Epoch 222/300, Loss: 0.1292\n",
      "Epoch 223/300, Loss: 0.1708\n",
      "Epoch 224/300, Loss: 0.1703\n",
      "Epoch 225/300, Loss: 0.1546\n",
      "Epoch 226/300, Loss: 0.1794\n",
      "Epoch 227/300, Loss: 0.1602\n",
      "Epoch 228/300, Loss: 0.1637\n",
      "Epoch 229/300, Loss: 0.1330\n",
      "Epoch 230/300, Loss: 0.1684\n",
      "Epoch 231/300, Loss: 0.1669\n",
      "Epoch 232/300, Loss: 0.1586\n",
      "Epoch 233/300, Loss: 0.1500\n",
      "Epoch 234/300, Loss: 0.1437\n",
      "Epoch 235/300, Loss: 0.1653\n",
      "Epoch 236/300, Loss: 0.1536\n",
      "Epoch 237/300, Loss: 0.1359\n",
      "Epoch 238/300, Loss: 0.1550\n",
      "Epoch 239/300, Loss: 0.1796\n",
      "Epoch 240/300, Loss: 0.1396\n",
      "Epoch 241/300, Loss: 0.1436\n",
      "Epoch 242/300, Loss: 0.1668\n",
      "Epoch 243/300, Loss: 0.1922\n",
      "Epoch 244/300, Loss: 0.1645\n",
      "Epoch 245/300, Loss: 0.1292\n",
      "Epoch 246/300, Loss: 0.1276\n",
      "Epoch 247/300, Loss: 0.1596\n",
      "Epoch 248/300, Loss: 0.1611\n",
      "Epoch 249/300, Loss: 0.1483\n",
      "Epoch 250/300, Loss: 0.1624\n",
      "Epoch 251/300, Loss: 0.1531\n",
      "Epoch 252/300, Loss: 0.1602\n",
      "Epoch 253/300, Loss: 0.0931\n",
      "Epoch 254/300, Loss: 0.1667\n",
      "Epoch 255/300, Loss: 0.1733\n",
      "Epoch 256/300, Loss: 0.1818\n",
      "Epoch 257/300, Loss: 0.1552\n",
      "Epoch 258/300, Loss: 0.1523\n",
      "Epoch 259/300, Loss: 0.1530\n",
      "Epoch 260/300, Loss: 0.1432\n",
      "Epoch 261/300, Loss: 0.1760\n",
      "Epoch 262/300, Loss: 0.1646\n",
      "Epoch 263/300, Loss: 0.1452\n",
      "Epoch 264/300, Loss: 0.1438\n",
      "Epoch 265/300, Loss: 0.1416\n",
      "Epoch 266/300, Loss: 0.1534\n",
      "Epoch 267/300, Loss: 0.1440\n",
      "Epoch 268/300, Loss: 0.1732\n",
      "Epoch 269/300, Loss: 0.1714\n",
      "Epoch 270/300, Loss: 0.1096\n",
      "Epoch 271/300, Loss: 0.1440\n",
      "Epoch 272/300, Loss: 0.1279\n",
      "Epoch 273/300, Loss: 0.1414\n",
      "Epoch 274/300, Loss: 0.1551\n",
      "Epoch 275/300, Loss: 0.1432\n",
      "Epoch 276/300, Loss: 0.1641\n",
      "Epoch 277/300, Loss: 0.1674\n",
      "Epoch 278/300, Loss: 0.1549\n",
      "Epoch 279/300, Loss: 0.1618\n",
      "Epoch 280/300, Loss: 0.1812\n",
      "Epoch 281/300, Loss: 0.1668\n",
      "Epoch 282/300, Loss: 0.1589\n",
      "Epoch 283/300, Loss: 0.1658\n",
      "Epoch 284/300, Loss: 0.1409\n",
      "Epoch 285/300, Loss: 0.1572\n",
      "Epoch 286/300, Loss: 0.1306\n",
      "Epoch 287/300, Loss: 0.1163\n",
      "Epoch 288/300, Loss: 0.1513\n",
      "Epoch 289/300, Loss: 0.1564\n",
      "Epoch 290/300, Loss: 0.1398\n",
      "Epoch 291/300, Loss: 0.1664\n",
      "Epoch 292/300, Loss: 0.1406\n",
      "Epoch 293/300, Loss: 0.1402\n",
      "Epoch 294/300, Loss: 0.1428\n",
      "Epoch 295/300, Loss: 0.1145\n",
      "Epoch 296/300, Loss: 0.1461\n",
      "Epoch 297/300, Loss: 0.1707\n",
      "Epoch 298/300, Loss: 0.1354\n",
      "Epoch 299/300, Loss: 0.1412\n",
      "Epoch 300/300, Loss: 0.1697\n"
     ]
    }
   ],
   "source": [
    "# Train the network with batch training\n",
    "nn.train(X_train, y_train, epochs=300, learning_rate=0.01, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9425\n"
     ]
    }
   ],
   "source": [
    "\n",
    "predictions = nn.predict(X_test)\n",
    "y_test_labels = np.argmax(y_test, axis=1)\n",
    "accuracy = np.mean(predictions == y_test_labels)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD_Momentum:\n",
    "    def __init__(self, learning_rate=0.01, momentum=0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.velocity = {}\n",
    "    \n",
    "    def initialize(self, params):\n",
    "        # Initialize velocity for each parameter\n",
    "        for layer_idx in range(len(params)):\n",
    "            self.velocity[f'weights_{layer_idx}'] = np.zeros_like(params[layer_idx]['weights'])\n",
    "            self.velocity[f'biases_{layer_idx}'] = np.zeros_like(params[layer_idx]['biases'])\n",
    "    \n",
    "    def update(self, params, gradients):\n",
    "        for layer_idx in range(len(params)):\n",
    "            # Update weights using momentum\n",
    "            self.velocity[f'weights_{layer_idx}'] = (self.momentum * self.velocity[f'weights_{layer_idx}'] - \n",
    "                                                   self.learning_rate * gradients[layer_idx]['d_weights'])\n",
    "            self.velocity[f'biases_{layer_idx}'] = (self.momentum * self.velocity[f'biases_{layer_idx}'] - \n",
    "                                                  self.learning_rate * gradients[layer_idx]['d_biases'])\n",
    "            \n",
    "            params[layer_idx]['weights'] += self.velocity[f'weights_{layer_idx}']\n",
    "            params[layer_idx]['biases'] += self.velocity[f'biases_{layer_idx}']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniBatchSGD:\n",
    "    def __init__(self, learning_rate=0.01, batch_size=32):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def initialize(self, params):\n",
    "        pass  # No initialization needed for basic SGD\n",
    "    \n",
    "    def update(self, params, gradients):\n",
    "        for layer_idx in range(len(params)):\n",
    "            params[layer_idx]['weights'] -= self.learning_rate * gradients[layer_idx]['d_weights']\n",
    "            params[layer_idx]['biases'] -= self.learning_rate * gradients[layer_idx]['d_biases']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Different Optimizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300, Loss: 0.7654\n",
      "Epoch 2/300, Loss: 0.6630\n",
      "Epoch 3/300, Loss: 0.6280\n",
      "Epoch 4/300, Loss: 0.5858\n",
      "Epoch 5/300, Loss: 0.5612\n",
      "Epoch 6/300, Loss: 0.5180\n",
      "Epoch 7/300, Loss: 0.4591\n",
      "Epoch 8/300, Loss: 0.3264\n",
      "Epoch 9/300, Loss: 0.2849\n",
      "Epoch 10/300, Loss: 0.2999\n",
      "Epoch 11/300, Loss: 0.2987\n",
      "Epoch 12/300, Loss: 0.3022\n",
      "Epoch 13/300, Loss: 0.2455\n",
      "Epoch 14/300, Loss: 0.2303\n",
      "Epoch 15/300, Loss: 0.3122\n",
      "Epoch 16/300, Loss: 0.2734\n",
      "Epoch 17/300, Loss: 0.1753\n",
      "Epoch 18/300, Loss: 0.2240\n",
      "Epoch 19/300, Loss: 0.2985\n",
      "Epoch 20/300, Loss: 0.2237\n",
      "Epoch 21/300, Loss: 0.1660\n",
      "Epoch 22/300, Loss: 0.2265\n",
      "Epoch 23/300, Loss: 0.2143\n",
      "Epoch 24/300, Loss: 0.2334\n",
      "Epoch 25/300, Loss: 0.2027\n",
      "Epoch 26/300, Loss: 0.1662\n",
      "Epoch 27/300, Loss: 0.2422\n",
      "Epoch 28/300, Loss: 0.2252\n",
      "Epoch 29/300, Loss: 0.1847\n",
      "Epoch 30/300, Loss: 0.1887\n",
      "Epoch 31/300, Loss: 0.1659\n",
      "Epoch 32/300, Loss: 0.2539\n",
      "Epoch 33/300, Loss: 0.1961\n",
      "Epoch 34/300, Loss: 0.2168\n",
      "Epoch 35/300, Loss: 0.1998\n",
      "Epoch 36/300, Loss: 0.1970\n",
      "Epoch 37/300, Loss: 0.1782\n",
      "Epoch 38/300, Loss: 0.1936\n",
      "Epoch 39/300, Loss: 0.1959\n",
      "Epoch 40/300, Loss: 0.2231\n",
      "Epoch 41/300, Loss: 0.1608\n",
      "Epoch 42/300, Loss: 0.1786\n",
      "Epoch 43/300, Loss: 0.1736\n",
      "Epoch 44/300, Loss: 0.1725\n",
      "Epoch 45/300, Loss: 0.1940\n",
      "Epoch 46/300, Loss: 0.1810\n",
      "Epoch 47/300, Loss: 0.1709\n",
      "Epoch 48/300, Loss: 0.1727\n",
      "Epoch 49/300, Loss: 0.1821\n",
      "Epoch 50/300, Loss: 0.1994\n",
      "Epoch 51/300, Loss: 0.2159\n",
      "Epoch 52/300, Loss: 0.1922\n",
      "Epoch 53/300, Loss: 0.1836\n",
      "Epoch 54/300, Loss: 0.1909\n",
      "Epoch 55/300, Loss: 0.2085\n",
      "Epoch 56/300, Loss: 0.2109\n",
      "Epoch 57/300, Loss: 0.1966\n",
      "Epoch 58/300, Loss: 0.2068\n",
      "Epoch 59/300, Loss: 0.1728\n",
      "Epoch 60/300, Loss: 0.1843\n",
      "Epoch 61/300, Loss: 0.2159\n",
      "Epoch 62/300, Loss: 0.2017\n",
      "Epoch 63/300, Loss: 0.2680\n",
      "Epoch 64/300, Loss: 0.1984\n",
      "Epoch 65/300, Loss: 0.2422\n",
      "Epoch 66/300, Loss: 0.1821\n",
      "Epoch 67/300, Loss: 0.1588\n",
      "Epoch 68/300, Loss: 0.2068\n",
      "Epoch 69/300, Loss: 0.2071\n",
      "Epoch 70/300, Loss: 0.1487\n",
      "Epoch 71/300, Loss: 0.1662\n",
      "Epoch 72/300, Loss: 0.1383\n",
      "Epoch 73/300, Loss: 0.2057\n",
      "Epoch 74/300, Loss: 0.1726\n",
      "Epoch 75/300, Loss: 0.2097\n",
      "Epoch 76/300, Loss: 0.1565\n",
      "Epoch 77/300, Loss: 0.1772\n",
      "Epoch 78/300, Loss: 0.1597\n",
      "Epoch 79/300, Loss: 0.1953\n",
      "Epoch 80/300, Loss: 0.2000\n",
      "Epoch 81/300, Loss: 0.1560\n",
      "Epoch 82/300, Loss: 0.1282\n",
      "Epoch 83/300, Loss: 0.2397\n",
      "Epoch 84/300, Loss: 0.1875\n",
      "Epoch 85/300, Loss: 0.1717\n",
      "Epoch 86/300, Loss: 0.1518\n",
      "Epoch 87/300, Loss: 0.1768\n",
      "Epoch 88/300, Loss: 0.1641\n",
      "Epoch 89/300, Loss: 0.1701\n",
      "Epoch 90/300, Loss: 0.1374\n",
      "Epoch 91/300, Loss: 0.2036\n",
      "Epoch 92/300, Loss: 0.1697\n",
      "Epoch 93/300, Loss: 0.1695\n",
      "Epoch 94/300, Loss: 0.1840\n",
      "Epoch 95/300, Loss: 0.1430\n",
      "Epoch 96/300, Loss: 0.1599\n",
      "Epoch 97/300, Loss: 0.1442\n",
      "Epoch 98/300, Loss: 0.1701\n",
      "Epoch 99/300, Loss: 0.1269\n",
      "Epoch 100/300, Loss: 0.1602\n",
      "Epoch 101/300, Loss: 0.1688\n",
      "Epoch 102/300, Loss: 0.1879\n",
      "Epoch 103/300, Loss: 0.1615\n",
      "Epoch 104/300, Loss: 0.1788\n",
      "Epoch 105/300, Loss: 0.2025\n",
      "Epoch 106/300, Loss: 0.1944\n",
      "Epoch 107/300, Loss: 0.1559\n",
      "Epoch 108/300, Loss: 0.1773\n",
      "Epoch 109/300, Loss: 0.1757\n",
      "Epoch 110/300, Loss: 0.1724\n",
      "Epoch 111/300, Loss: 0.1454\n",
      "Epoch 112/300, Loss: 0.1370\n",
      "Epoch 113/300, Loss: 0.1390\n",
      "Epoch 114/300, Loss: 0.1425\n",
      "Epoch 115/300, Loss: 0.1728\n",
      "Epoch 116/300, Loss: 0.1526\n",
      "Epoch 117/300, Loss: 0.1838\n",
      "Epoch 118/300, Loss: 0.1693\n",
      "Epoch 119/300, Loss: 0.1416\n",
      "Epoch 120/300, Loss: 0.1508\n",
      "Epoch 121/300, Loss: 0.1377\n",
      "Epoch 122/300, Loss: 0.1582\n",
      "Epoch 123/300, Loss: 0.1576\n",
      "Epoch 124/300, Loss: 0.1704\n",
      "Epoch 125/300, Loss: 0.1897\n",
      "Epoch 126/300, Loss: 0.1741\n",
      "Epoch 127/300, Loss: 0.1740\n",
      "Epoch 128/300, Loss: 0.1572\n",
      "Epoch 129/300, Loss: 0.2086\n",
      "Epoch 130/300, Loss: 0.1904\n",
      "Epoch 131/300, Loss: 0.1677\n",
      "Epoch 132/300, Loss: 0.1470\n",
      "Epoch 133/300, Loss: 0.1622\n",
      "Epoch 134/300, Loss: 0.1857\n",
      "Epoch 135/300, Loss: 0.1644\n",
      "Epoch 136/300, Loss: 0.1684\n",
      "Epoch 137/300, Loss: 0.1374\n",
      "Epoch 138/300, Loss: 0.1381\n",
      "Epoch 139/300, Loss: 0.1612\n",
      "Epoch 140/300, Loss: 0.1445\n",
      "Epoch 141/300, Loss: 0.1941\n",
      "Epoch 142/300, Loss: 0.1644\n",
      "Epoch 143/300, Loss: 0.1743\n",
      "Epoch 144/300, Loss: 0.1821\n",
      "Epoch 145/300, Loss: 0.1672\n",
      "Epoch 146/300, Loss: 0.1622\n",
      "Epoch 147/300, Loss: 0.1573\n",
      "Epoch 148/300, Loss: 0.1314\n",
      "Epoch 149/300, Loss: 0.1864\n",
      "Epoch 150/300, Loss: 0.2093\n",
      "Epoch 151/300, Loss: 0.1572\n",
      "Epoch 152/300, Loss: 0.1793\n",
      "Epoch 153/300, Loss: 0.1495\n",
      "Epoch 154/300, Loss: 0.2075\n",
      "Epoch 155/300, Loss: 0.1529\n",
      "Epoch 156/300, Loss: 0.1838\n",
      "Epoch 157/300, Loss: 0.1608\n",
      "Epoch 158/300, Loss: 0.1509\n",
      "Epoch 159/300, Loss: 0.1321\n",
      "Epoch 160/300, Loss: 0.1672\n",
      "Epoch 161/300, Loss: 0.1958\n",
      "Epoch 162/300, Loss: 0.1514\n",
      "Epoch 163/300, Loss: 0.2092\n",
      "Epoch 164/300, Loss: 0.1767\n",
      "Epoch 165/300, Loss: 0.1714\n",
      "Epoch 166/300, Loss: 0.1745\n",
      "Epoch 167/300, Loss: 0.1742\n",
      "Epoch 168/300, Loss: 0.1458\n",
      "Epoch 169/300, Loss: 0.1898\n",
      "Epoch 170/300, Loss: 0.2111\n",
      "Epoch 171/300, Loss: 0.1712\n",
      "Epoch 172/300, Loss: 0.1933\n",
      "Epoch 173/300, Loss: 0.1435\n",
      "Epoch 174/300, Loss: 0.1762\n",
      "Epoch 175/300, Loss: 0.1633\n",
      "Epoch 176/300, Loss: 0.1943\n",
      "Epoch 177/300, Loss: 0.1599\n",
      "Epoch 178/300, Loss: 0.1522\n",
      "Epoch 179/300, Loss: 0.1746\n",
      "Epoch 180/300, Loss: 0.1511\n",
      "Epoch 181/300, Loss: 0.1744\n",
      "Epoch 182/300, Loss: 0.1568\n",
      "Epoch 183/300, Loss: 0.1472\n",
      "Epoch 184/300, Loss: 0.1560\n",
      "Epoch 185/300, Loss: 0.1709\n",
      "Epoch 186/300, Loss: 0.1483\n",
      "Epoch 187/300, Loss: 0.1673\n",
      "Epoch 188/300, Loss: 0.1662\n",
      "Epoch 189/300, Loss: 0.1392\n",
      "Epoch 190/300, Loss: 0.1524\n",
      "Epoch 191/300, Loss: 0.1669\n",
      "Epoch 192/300, Loss: 0.1428\n",
      "Epoch 193/300, Loss: 0.1888\n",
      "Epoch 194/300, Loss: 0.1424\n",
      "Epoch 195/300, Loss: 0.1526\n",
      "Epoch 196/300, Loss: 0.1439\n",
      "Epoch 197/300, Loss: 0.1803\n",
      "Epoch 198/300, Loss: 0.1968\n",
      "Epoch 199/300, Loss: 0.1776\n",
      "Epoch 200/300, Loss: 0.1778\n",
      "Epoch 201/300, Loss: 0.1544\n",
      "Epoch 202/300, Loss: 0.1707\n",
      "Epoch 203/300, Loss: 0.1712\n",
      "Epoch 204/300, Loss: 0.1571\n",
      "Epoch 205/300, Loss: 0.1764\n",
      "Epoch 206/300, Loss: 0.1428\n",
      "Epoch 207/300, Loss: 0.1786\n",
      "Epoch 208/300, Loss: 0.1614\n",
      "Epoch 209/300, Loss: 0.1580\n",
      "Epoch 210/300, Loss: 0.1602\n",
      "Epoch 211/300, Loss: 0.1746\n",
      "Epoch 212/300, Loss: 0.1680\n",
      "Epoch 213/300, Loss: 0.1614\n",
      "Epoch 214/300, Loss: 0.1387\n",
      "Epoch 215/300, Loss: 0.1523\n",
      "Epoch 216/300, Loss: 0.1735\n",
      "Epoch 217/300, Loss: 0.2009\n",
      "Epoch 218/300, Loss: 0.1567\n",
      "Epoch 219/300, Loss: 0.1871\n",
      "Epoch 220/300, Loss: 0.1414\n",
      "Epoch 221/300, Loss: 0.1620\n",
      "Epoch 222/300, Loss: 0.1622\n",
      "Epoch 223/300, Loss: 0.1482\n",
      "Epoch 224/300, Loss: 0.1867\n",
      "Epoch 225/300, Loss: 0.1564\n",
      "Epoch 226/300, Loss: 0.1911\n",
      "Epoch 227/300, Loss: 0.1586\n",
      "Epoch 228/300, Loss: 0.1664\n",
      "Epoch 229/300, Loss: 0.1685\n",
      "Epoch 230/300, Loss: 0.1493\n",
      "Epoch 231/300, Loss: 0.1774\n",
      "Epoch 232/300, Loss: 0.1902\n",
      "Epoch 233/300, Loss: 0.1322\n",
      "Epoch 234/300, Loss: 0.1436\n",
      "Epoch 235/300, Loss: 0.1397\n",
      "Epoch 236/300, Loss: 0.2084\n",
      "Epoch 237/300, Loss: 0.1806\n",
      "Epoch 238/300, Loss: 0.1601\n",
      "Epoch 239/300, Loss: 0.1513\n",
      "Epoch 240/300, Loss: 0.1625\n",
      "Epoch 241/300, Loss: 0.1672\n",
      "Epoch 242/300, Loss: 0.1504\n",
      "Epoch 243/300, Loss: 0.1667\n",
      "Epoch 244/300, Loss: 0.1575\n",
      "Epoch 245/300, Loss: 0.1419\n",
      "Epoch 246/300, Loss: 0.1575\n",
      "Epoch 247/300, Loss: 0.1822\n",
      "Epoch 248/300, Loss: 0.1470\n",
      "Epoch 249/300, Loss: 0.1435\n",
      "Epoch 250/300, Loss: 0.1402\n",
      "Epoch 251/300, Loss: 0.1429\n",
      "Epoch 252/300, Loss: 0.1717\n",
      "Epoch 253/300, Loss: 0.1513\n",
      "Epoch 254/300, Loss: 0.1527\n",
      "Epoch 255/300, Loss: 0.1631\n",
      "Epoch 256/300, Loss: 0.1428\n",
      "Epoch 257/300, Loss: 0.1460\n",
      "Epoch 258/300, Loss: 0.1901\n",
      "Epoch 259/300, Loss: 0.1504\n",
      "Epoch 260/300, Loss: 0.1695\n",
      "Epoch 261/300, Loss: 0.1560\n",
      "Epoch 262/300, Loss: 0.1531\n",
      "Epoch 263/300, Loss: 0.1222\n",
      "Epoch 264/300, Loss: 0.1705\n",
      "Epoch 265/300, Loss: 0.1561\n",
      "Epoch 266/300, Loss: 0.1599\n",
      "Epoch 267/300, Loss: 0.1706\n",
      "Epoch 268/300, Loss: 0.1721\n",
      "Epoch 269/300, Loss: 0.1581\n",
      "Epoch 270/300, Loss: 0.1599\n",
      "Epoch 271/300, Loss: 0.1723\n",
      "Epoch 272/300, Loss: 0.1264\n",
      "Epoch 273/300, Loss: 0.1483\n",
      "Epoch 274/300, Loss: 0.1715\n",
      "Epoch 275/300, Loss: 0.1466\n",
      "Epoch 276/300, Loss: 0.1640\n",
      "Epoch 277/300, Loss: 0.1660\n",
      "Epoch 278/300, Loss: 0.1541\n",
      "Epoch 279/300, Loss: 0.1888\n",
      "Epoch 280/300, Loss: 0.1566\n",
      "Epoch 281/300, Loss: 0.1414\n",
      "Epoch 282/300, Loss: 0.1528\n",
      "Epoch 283/300, Loss: 0.1543\n",
      "Epoch 284/300, Loss: 0.1905\n",
      "Epoch 285/300, Loss: 0.1583\n",
      "Epoch 286/300, Loss: 0.1548\n",
      "Epoch 287/300, Loss: 0.1295\n",
      "Epoch 288/300, Loss: 0.1638\n",
      "Epoch 289/300, Loss: 0.1469\n",
      "Epoch 290/300, Loss: 0.1364\n",
      "Epoch 291/300, Loss: 0.1580\n",
      "Epoch 292/300, Loss: 0.1663\n",
      "Epoch 293/300, Loss: 0.1814\n",
      "Epoch 294/300, Loss: 0.1913\n",
      "Epoch 295/300, Loss: 0.1472\n",
      "Epoch 296/300, Loss: 0.1597\n",
      "Epoch 297/300, Loss: 0.1178\n",
      "Epoch 298/300, Loss: 0.1369\n",
      "Epoch 299/300, Loss: 0.1439\n",
      "Epoch 300/300, Loss: 0.1733\n"
     ]
    }
   ],
   "source": [
    "# Test with SGD Momentum\n",
    "nn_momentum = Neural_network(optimizer_type='sgd_momentum', learning_rate=0.01, momentum=0.9)\n",
    "nn_momentum.add_layer(Dense_layer(X_train.shape[1], 16), Relu(), Dropout(0.05))\n",
    "nn_momentum.add_layer(Dense_layer(16, 8), Relu(), Dropout(0.05))\n",
    "nn_momentum.add_layer(Dense_layer(8, 4), Softmax())\n",
    "nn_momentum.train(X_train, y_train, epochs=300, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy with SGD Momentum: 0.9381\n"
     ]
    }
   ],
   "source": [
    "# Get accuracy\n",
    "predictions = nn_momentum.predict(X_test)\n",
    "y_test_labels = np.argmax(y_test, axis=1)\n",
    "accuracy = np.mean(predictions == y_test_labels)\n",
    "print(f\"Test Accuracy with SGD Momentum: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300, Loss: 1.1176\n",
      "Epoch 2/300, Loss: 1.1510\n",
      "Epoch 3/300, Loss: 0.8142\n",
      "Epoch 4/300, Loss: 0.8686\n",
      "Epoch 5/300, Loss: 0.8255\n",
      "Epoch 6/300, Loss: 0.7549\n",
      "Epoch 7/300, Loss: 0.7522\n",
      "Epoch 8/300, Loss: 0.7283\n",
      "Epoch 9/300, Loss: 0.7151\n",
      "Epoch 10/300, Loss: 0.6941\n",
      "Epoch 11/300, Loss: 0.6896\n",
      "Epoch 12/300, Loss: 0.6576\n",
      "Epoch 13/300, Loss: 0.6400\n",
      "Epoch 14/300, Loss: 0.6154\n",
      "Epoch 15/300, Loss: 0.6089\n",
      "Epoch 16/300, Loss: 0.6182\n",
      "Epoch 17/300, Loss: 0.6160\n",
      "Epoch 18/300, Loss: 0.5965\n",
      "Epoch 19/300, Loss: 0.5873\n",
      "Epoch 20/300, Loss: 0.5769\n",
      "Epoch 21/300, Loss: 0.5477\n",
      "Epoch 22/300, Loss: 0.5282\n",
      "Epoch 23/300, Loss: 0.4986\n",
      "Epoch 24/300, Loss: 0.4588\n",
      "Epoch 25/300, Loss: 0.4275\n",
      "Epoch 26/300, Loss: 0.3943\n",
      "Epoch 27/300, Loss: 0.4119\n",
      "Epoch 28/300, Loss: 0.3535\n",
      "Epoch 29/300, Loss: 0.3658\n",
      "Epoch 30/300, Loss: 0.3199\n",
      "Epoch 31/300, Loss: 0.3453\n",
      "Epoch 32/300, Loss: 0.3549\n",
      "Epoch 33/300, Loss: 0.2976\n",
      "Epoch 34/300, Loss: 0.3259\n",
      "Epoch 35/300, Loss: 0.2600\n",
      "Epoch 36/300, Loss: 0.2694\n",
      "Epoch 37/300, Loss: 0.3095\n",
      "Epoch 38/300, Loss: 0.2968\n",
      "Epoch 39/300, Loss: 0.2789\n",
      "Epoch 40/300, Loss: 0.2625\n",
      "Epoch 41/300, Loss: 0.2705\n",
      "Epoch 42/300, Loss: 0.2948\n",
      "Epoch 43/300, Loss: 0.3393\n",
      "Epoch 44/300, Loss: 0.2310\n",
      "Epoch 45/300, Loss: 0.2883\n",
      "Epoch 46/300, Loss: 0.2860\n",
      "Epoch 47/300, Loss: 0.2281\n",
      "Epoch 48/300, Loss: 0.2955\n",
      "Epoch 49/300, Loss: 0.2747\n",
      "Epoch 50/300, Loss: 0.2344\n",
      "Epoch 51/300, Loss: 0.2185\n",
      "Epoch 52/300, Loss: 0.2630\n",
      "Epoch 53/300, Loss: 0.2123\n",
      "Epoch 54/300, Loss: 0.2672\n",
      "Epoch 55/300, Loss: 0.2327\n",
      "Epoch 56/300, Loss: 0.2219\n",
      "Epoch 57/300, Loss: 0.2418\n",
      "Epoch 58/300, Loss: 0.2487\n",
      "Epoch 59/300, Loss: 0.2594\n",
      "Epoch 60/300, Loss: 0.2398\n",
      "Epoch 61/300, Loss: 0.2241\n",
      "Epoch 62/300, Loss: 0.2444\n",
      "Epoch 63/300, Loss: 0.1891\n",
      "Epoch 64/300, Loss: 0.2766\n",
      "Epoch 65/300, Loss: 0.2269\n",
      "Epoch 66/300, Loss: 0.1991\n",
      "Epoch 67/300, Loss: 0.2263\n",
      "Epoch 68/300, Loss: 0.2326\n",
      "Epoch 69/300, Loss: 0.2507\n",
      "Epoch 70/300, Loss: 0.2992\n",
      "Epoch 71/300, Loss: 0.2330\n",
      "Epoch 72/300, Loss: 0.2140\n",
      "Epoch 73/300, Loss: 0.2427\n",
      "Epoch 74/300, Loss: 0.2027\n",
      "Epoch 75/300, Loss: 0.2338\n",
      "Epoch 76/300, Loss: 0.2215\n",
      "Epoch 77/300, Loss: 0.1831\n",
      "Epoch 78/300, Loss: 0.2024\n",
      "Epoch 79/300, Loss: 0.1984\n",
      "Epoch 80/300, Loss: 0.2172\n",
      "Epoch 81/300, Loss: 0.2246\n",
      "Epoch 82/300, Loss: 0.2050\n",
      "Epoch 83/300, Loss: 0.2611\n",
      "Epoch 84/300, Loss: 0.2514\n",
      "Epoch 85/300, Loss: 0.2251\n",
      "Epoch 86/300, Loss: 0.1875\n",
      "Epoch 87/300, Loss: 0.1973\n",
      "Epoch 88/300, Loss: 0.2495\n",
      "Epoch 89/300, Loss: 0.1995\n",
      "Epoch 90/300, Loss: 0.1665\n",
      "Epoch 91/300, Loss: 0.1783\n",
      "Epoch 92/300, Loss: 0.1580\n",
      "Epoch 93/300, Loss: 0.1964\n",
      "Epoch 94/300, Loss: 0.1940\n",
      "Epoch 95/300, Loss: 0.2246\n",
      "Epoch 96/300, Loss: 0.1981\n",
      "Epoch 97/300, Loss: 0.1862\n",
      "Epoch 98/300, Loss: 0.2761\n",
      "Epoch 99/300, Loss: 0.2012\n",
      "Epoch 100/300, Loss: 0.2291\n",
      "Epoch 101/300, Loss: 0.2031\n",
      "Epoch 102/300, Loss: 0.1870\n",
      "Epoch 103/300, Loss: 0.2345\n",
      "Epoch 104/300, Loss: 0.2289\n",
      "Epoch 105/300, Loss: 0.2011\n",
      "Epoch 106/300, Loss: 0.2181\n",
      "Epoch 107/300, Loss: 0.2459\n",
      "Epoch 108/300, Loss: 0.1948\n",
      "Epoch 109/300, Loss: 0.1797\n",
      "Epoch 110/300, Loss: 0.1923\n",
      "Epoch 111/300, Loss: 0.2159\n",
      "Epoch 112/300, Loss: 0.2203\n",
      "Epoch 113/300, Loss: 0.2581\n",
      "Epoch 114/300, Loss: 0.2047\n",
      "Epoch 115/300, Loss: 0.1272\n",
      "Epoch 116/300, Loss: 0.2088\n",
      "Epoch 117/300, Loss: 0.2282\n",
      "Epoch 118/300, Loss: 0.1807\n",
      "Epoch 119/300, Loss: 0.1556\n",
      "Epoch 120/300, Loss: 0.1912\n",
      "Epoch 121/300, Loss: 0.1744\n",
      "Epoch 122/300, Loss: 0.2729\n",
      "Epoch 123/300, Loss: 0.2397\n",
      "Epoch 124/300, Loss: 0.1807\n",
      "Epoch 125/300, Loss: 0.1828\n",
      "Epoch 126/300, Loss: 0.1912\n",
      "Epoch 127/300, Loss: 0.2090\n",
      "Epoch 128/300, Loss: 0.2013\n",
      "Epoch 129/300, Loss: 0.1852\n",
      "Epoch 130/300, Loss: 0.2148\n",
      "Epoch 131/300, Loss: 0.1838\n",
      "Epoch 132/300, Loss: 0.2192\n",
      "Epoch 133/300, Loss: 0.1922\n",
      "Epoch 134/300, Loss: 0.1866\n",
      "Epoch 135/300, Loss: 0.1905\n",
      "Epoch 136/300, Loss: 0.1963\n",
      "Epoch 137/300, Loss: 0.2073\n",
      "Epoch 138/300, Loss: 0.1913\n",
      "Epoch 139/300, Loss: 0.2168\n",
      "Epoch 140/300, Loss: 0.2610\n",
      "Epoch 141/300, Loss: 0.2148\n",
      "Epoch 142/300, Loss: 0.2217\n",
      "Epoch 143/300, Loss: 0.1887\n",
      "Epoch 144/300, Loss: 0.1444\n",
      "Epoch 145/300, Loss: 0.2517\n",
      "Epoch 146/300, Loss: 0.1693\n",
      "Epoch 147/300, Loss: 0.2436\n",
      "Epoch 148/300, Loss: 0.2123\n",
      "Epoch 149/300, Loss: 0.2035\n",
      "Epoch 150/300, Loss: 0.1757\n",
      "Epoch 151/300, Loss: 0.2246\n",
      "Epoch 152/300, Loss: 0.1862\n",
      "Epoch 153/300, Loss: 0.1960\n",
      "Epoch 154/300, Loss: 0.1935\n",
      "Epoch 155/300, Loss: 0.2274\n",
      "Epoch 156/300, Loss: 0.1874\n",
      "Epoch 157/300, Loss: 0.2011\n",
      "Epoch 158/300, Loss: 0.2256\n",
      "Epoch 159/300, Loss: 0.2010\n",
      "Epoch 160/300, Loss: 0.1742\n",
      "Epoch 161/300, Loss: 0.1609\n",
      "Epoch 162/300, Loss: 0.1812\n",
      "Epoch 163/300, Loss: 0.1527\n",
      "Epoch 164/300, Loss: 0.1355\n",
      "Epoch 165/300, Loss: 0.2040\n",
      "Epoch 166/300, Loss: 0.2032\n",
      "Epoch 167/300, Loss: 0.1636\n",
      "Epoch 168/300, Loss: 0.2049\n",
      "Epoch 169/300, Loss: 0.1706\n",
      "Epoch 170/300, Loss: 0.2519\n",
      "Epoch 171/300, Loss: 0.1884\n",
      "Epoch 172/300, Loss: 0.1769\n",
      "Epoch 173/300, Loss: 0.1731\n",
      "Epoch 174/300, Loss: 0.1852\n",
      "Epoch 175/300, Loss: 0.2014\n",
      "Epoch 176/300, Loss: 0.1868\n",
      "Epoch 177/300, Loss: 0.1887\n",
      "Epoch 178/300, Loss: 0.2020\n",
      "Epoch 179/300, Loss: 0.1992\n",
      "Epoch 180/300, Loss: 0.1753\n",
      "Epoch 181/300, Loss: 0.1574\n",
      "Epoch 182/300, Loss: 0.1406\n",
      "Epoch 183/300, Loss: 0.1909\n",
      "Epoch 184/300, Loss: 0.1733\n",
      "Epoch 185/300, Loss: 0.1905\n",
      "Epoch 186/300, Loss: 0.1791\n",
      "Epoch 187/300, Loss: 0.2531\n",
      "Epoch 188/300, Loss: 0.1668\n",
      "Epoch 189/300, Loss: 0.1721\n",
      "Epoch 190/300, Loss: 0.1868\n",
      "Epoch 191/300, Loss: 0.1598\n",
      "Epoch 192/300, Loss: 0.2109\n",
      "Epoch 193/300, Loss: 0.1619\n",
      "Epoch 194/300, Loss: 0.1690\n",
      "Epoch 195/300, Loss: 0.1864\n",
      "Epoch 196/300, Loss: 0.1938\n",
      "Epoch 197/300, Loss: 0.1956\n",
      "Epoch 198/300, Loss: 0.1656\n",
      "Epoch 199/300, Loss: 0.2509\n",
      "Epoch 200/300, Loss: 0.1879\n",
      "Epoch 201/300, Loss: 0.2180\n",
      "Epoch 202/300, Loss: 0.1924\n",
      "Epoch 203/300, Loss: 0.2104\n",
      "Epoch 204/300, Loss: 0.1933\n",
      "Epoch 205/300, Loss: 0.2091\n",
      "Epoch 206/300, Loss: 0.1979\n",
      "Epoch 207/300, Loss: 0.1534\n",
      "Epoch 208/300, Loss: 0.1600\n",
      "Epoch 209/300, Loss: 0.1439\n",
      "Epoch 210/300, Loss: 0.1753\n",
      "Epoch 211/300, Loss: 0.1847\n",
      "Epoch 212/300, Loss: 0.1864\n",
      "Epoch 213/300, Loss: 0.2342\n",
      "Epoch 214/300, Loss: 0.1692\n",
      "Epoch 215/300, Loss: 0.1642\n",
      "Epoch 216/300, Loss: 0.1803\n",
      "Epoch 217/300, Loss: 0.1852\n",
      "Epoch 218/300, Loss: 0.1818\n",
      "Epoch 219/300, Loss: 0.1575\n",
      "Epoch 220/300, Loss: 0.1370\n",
      "Epoch 221/300, Loss: 0.1897\n",
      "Epoch 222/300, Loss: 0.1820\n",
      "Epoch 223/300, Loss: 0.1926\n",
      "Epoch 224/300, Loss: 0.1809\n",
      "Epoch 225/300, Loss: 0.1434\n",
      "Epoch 226/300, Loss: 0.1817\n",
      "Epoch 227/300, Loss: 0.1931\n",
      "Epoch 228/300, Loss: 0.2026\n",
      "Epoch 229/300, Loss: 0.1395\n",
      "Epoch 230/300, Loss: 0.1594\n",
      "Epoch 231/300, Loss: 0.1773\n",
      "Epoch 232/300, Loss: 0.1733\n",
      "Epoch 233/300, Loss: 0.1686\n",
      "Epoch 234/300, Loss: 0.2027\n",
      "Epoch 235/300, Loss: 0.1588\n",
      "Epoch 236/300, Loss: 0.2429\n",
      "Epoch 237/300, Loss: 0.1836\n",
      "Epoch 238/300, Loss: 0.1960\n",
      "Epoch 239/300, Loss: 0.1735\n",
      "Epoch 240/300, Loss: 0.1527\n",
      "Epoch 241/300, Loss: 0.1871\n",
      "Epoch 242/300, Loss: 0.1809\n",
      "Epoch 243/300, Loss: 0.1782\n",
      "Epoch 244/300, Loss: 0.1596\n",
      "Epoch 245/300, Loss: 0.2178\n",
      "Epoch 246/300, Loss: 0.1871\n",
      "Epoch 247/300, Loss: 0.1851\n",
      "Epoch 248/300, Loss: 0.1895\n",
      "Epoch 249/300, Loss: 0.1343\n",
      "Epoch 250/300, Loss: 0.2092\n",
      "Epoch 251/300, Loss: 0.2172\n",
      "Epoch 252/300, Loss: 0.1914\n",
      "Epoch 253/300, Loss: 0.1781\n",
      "Epoch 254/300, Loss: 0.1898\n",
      "Epoch 255/300, Loss: 0.1592\n",
      "Epoch 256/300, Loss: 0.2055\n",
      "Epoch 257/300, Loss: 0.2145\n",
      "Epoch 258/300, Loss: 0.1959\n",
      "Epoch 259/300, Loss: 0.2186\n",
      "Epoch 260/300, Loss: 0.1703\n",
      "Epoch 261/300, Loss: 0.1705\n",
      "Epoch 262/300, Loss: 0.2326\n",
      "Epoch 263/300, Loss: 0.2135\n",
      "Epoch 264/300, Loss: 0.1757\n",
      "Epoch 265/300, Loss: 0.1528\n",
      "Epoch 266/300, Loss: 0.1735\n",
      "Epoch 267/300, Loss: 0.2187\n",
      "Epoch 268/300, Loss: 0.1788\n",
      "Epoch 269/300, Loss: 0.1585\n",
      "Epoch 270/300, Loss: 0.1971\n",
      "Epoch 271/300, Loss: 0.2059\n",
      "Epoch 272/300, Loss: 0.1975\n",
      "Epoch 273/300, Loss: 0.1927\n",
      "Epoch 274/300, Loss: 0.2180\n",
      "Epoch 275/300, Loss: 0.1241\n",
      "Epoch 276/300, Loss: 0.1625\n",
      "Epoch 277/300, Loss: 0.1881\n",
      "Epoch 278/300, Loss: 0.1876\n",
      "Epoch 279/300, Loss: 0.1665\n",
      "Epoch 280/300, Loss: 0.1807\n",
      "Epoch 281/300, Loss: 0.1567\n",
      "Epoch 282/300, Loss: 0.1795\n",
      "Epoch 283/300, Loss: 0.1961\n",
      "Epoch 284/300, Loss: 0.1459\n",
      "Epoch 285/300, Loss: 0.1813\n",
      "Epoch 286/300, Loss: 0.1758\n",
      "Epoch 287/300, Loss: 0.1997\n",
      "Epoch 288/300, Loss: 0.1537\n",
      "Epoch 289/300, Loss: 0.1983\n",
      "Epoch 290/300, Loss: 0.1930\n",
      "Epoch 291/300, Loss: 0.2325\n",
      "Epoch 292/300, Loss: 0.2135\n",
      "Epoch 293/300, Loss: 0.1706\n",
      "Epoch 294/300, Loss: 0.1661\n",
      "Epoch 295/300, Loss: 0.1550\n",
      "Epoch 296/300, Loss: 0.2079\n",
      "Epoch 297/300, Loss: 0.1823\n",
      "Epoch 298/300, Loss: 0.2406\n",
      "Epoch 299/300, Loss: 0.1557\n",
      "Epoch 300/300, Loss: 0.1912\n"
     ]
    }
   ],
   "source": [
    "# Test with Mini-batch SGD\n",
    "nn_minibatch = Neural_network(optimizer_type='minibatch_sgd', learning_rate=0.01, batch_size=32)\n",
    "nn_minibatch.add_layer(Dense_layer(X_train.shape[1], 16), Relu(), Dropout(0.05))\n",
    "nn_minibatch.add_layer(Dense_layer(16, 8), Relu(), Dropout(0.05))\n",
    "nn_minibatch.add_layer(Dense_layer(8, 4), Softmax())\n",
    "nn_minibatch.train(X_train, y_train, epochs=300, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy with Mini-batch SGD: 0.9331\n"
     ]
    }
   ],
   "source": [
    "# Get accuracy\n",
    "predictions = nn_minibatch.predict(X_test)\n",
    "accuracy = np.mean(predictions == y_test_labels)\n",
    "print(f\"Test Accuracy with Mini-batch SGD: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data visualization "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
