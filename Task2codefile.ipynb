{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\Users\\Dell\\Desktop\\Task2Code\\INM702\n",
      "Contents of current directory: ['.git', 'coursework_task_1.ipynb', 'engine_data.csv', 'main.ipynb', 'README.md', 'Task2codefile.ipynb', 'updated_pollution_dataset.csv']\n"
     ]
    }
   ],
   "source": [
    "# Print current directory and contents\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "print(\"Contents of current directory:\", os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = r'c:\\Users\\Dell\\Desktop\\Task2Code'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset paths \n",
    "train_path = os.path.join('Train', 'Train')\n",
    "valid_path = os.path.join('Validation', 'Validation')\n",
    "test_path = os.path.join('Test', 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor()  \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset paths\n",
    "train_path = os.path.join(base_dir, 'Train', 'Train')\n",
    "valid_path = os.path.join(base_dir, 'Validation', 'Validation')\n",
    "test_path = os.path.join(base_dir, 'Test', 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create datasets\n",
    "train_dataset = datasets.ImageFolder(train_path, transform=transform)\n",
    "valid_dataset = datasets.ImageFolder(valid_path, transform=transform)\n",
    "test_dataset = datasets.ImageFolder(test_path, transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images: 1322\n",
      "Number of validation images: 60\n",
      "Number of test images: 150\n",
      "Classes: ['Healthy', 'Powdery', 'Rust']\n"
     ]
    }
   ],
   "source": [
    "# Print dataset information\n",
    "print(f'Number of training images: {len(train_dataset)}')\n",
    "print(f'Number of validation images: {len(valid_dataset)}')\n",
    "print(f'Number of test images: {len(test_dataset)}')\n",
    "print(f'Classes: {train_dataset.classes}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BaseLine Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaselineCNN, self).__init__()\n",
    "        # Just one conv layer\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Two fully connected layers\n",
    "        self.fc1 = nn.Linear(16 * 16 * 16, 64)\n",
    "        self.fc2 = nn.Linear(64, 3)  # 3 classes\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = x.view(-1, 16 * 16 * 16)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and training components\n",
    "model = BaselineCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1/5:\n",
      "Loss: 0.830\n",
      "Train Accuracy: 65.20%\n",
      "Validation Accuracy: 46.67%\n",
      "\n",
      "Epoch 2/5:\n",
      "Loss: 0.773\n",
      "Train Accuracy: 68.15%\n",
      "Validation Accuracy: 46.67%\n",
      "\n",
      "Epoch 3/5:\n",
      "Loss: 0.762\n",
      "Train Accuracy: 68.91%\n",
      "Validation Accuracy: 35.00%\n",
      "\n",
      "Epoch 4/5:\n",
      "Loss: 0.706\n",
      "Train Accuracy: 71.86%\n",
      "Validation Accuracy: 46.67%\n",
      "\n",
      "Epoch 5/5:\n",
      "Loss: 0.713\n",
      "Train Accuracy: 69.29%\n",
      "Validation Accuracy: 58.33%\n",
      "\n",
      "Test Accuracy: 56.67%\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Simple training components\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "print(\"Starting training...\")\n",
    "num_epochs = 5 \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    train_acc = 100. * correct / total\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in valid_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    val_acc = 100. * correct / total\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "    print(f'Loss: {running_loss/len(train_loader):.3f}')\n",
    "    print(f'Train Accuracy: {train_acc:.2f}%')\n",
    "    print(f'Validation Accuracy: {val_acc:.2f}%\\n')\n",
    "\n",
    "# Simple evaluation\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "print(f'Test Accuracy: {100.*correct/total:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 56.67%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "test_accuracy = 100. * correct / total \n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Test Accuracy: 56.67%\n"
     ]
    }
   ],
   "source": [
    "baseline_results = {\n",
    "    'test_accuracy': test_accuracy,  \n",
    "    'model_state': model.state_dict().copy()\n",
    "}\n",
    "\n",
    "\n",
    "print(f\"Baseline Test Accuracy: {float(baseline_results['test_accuracy']):.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "baseline_transform = transform  \n",
    "baseline_model = model  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Improvement 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Larger input size (224x224) for better feature detection\n",
    "- Data augmentation with random flips for better generalization\n",
    "- ImageNet normalization for training stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),          # Increased from 32x32\n",
    "    transforms.RandomHorizontalFlip(),      # New: data augmentation\n",
    "    transforms.ToTensor(),                  \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                       std=[0.229, 0.224, 0.225])  # Added normalization\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated datasets with new transform\n",
    "train_dataset = datasets.ImageFolder(train_path, transform=transform)\n",
    "valid_dataset = datasets.ImageFolder(valid_path, transform=transform)\n",
    "test_dataset = datasets.ImageFolder(test_path, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPROVEMENT 2: Enhanced CNN Architecture\n",
    "Changes from your advanced model:\n",
    "- Three convolutional layers instead of one\n",
    "- Added padding to maintain spatial dimensions\n",
    "- Increased number of filters (32, 64, 128)\n",
    "- Deeper network for better feature learning\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Improvement 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Three convolutional layers instead of one\n",
    "- Added padding to maintain spatial dimensions\n",
    "- Increased number of filters (32, 64, 128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlantDiseaseCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(PlantDiseaseCNN, self).__init__()\n",
    "        \n",
    "         \n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(128 * 28 * 28, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = self.pool(torch.relu(self.conv3(x)))\n",
    "        \n",
    "        x = x.view(-1, 128 * 28 * 28)\n",
    "        \n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Initialize model with improvements\n",
    "num_classes = len(train_dataset.classes)\n",
    "model = PlantDiseaseCNN(num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement improvement 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Using Adam optimizer instead of SGD\n",
    "- Added dropout for regularization\n",
    "- More epochs (10 instead of 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1/10\n",
      "Loss: 1.1100\n",
      "Train Accuracy: 59.30%\n",
      "Valid Accuracy: 61.67%\n",
      "\n",
      "Epoch 2/10\n",
      "Loss: 0.4233\n",
      "Train Accuracy: 83.96%\n",
      "Valid Accuracy: 81.67%\n",
      "\n",
      "Epoch 3/10\n",
      "Loss: 0.2438\n",
      "Train Accuracy: 91.45%\n",
      "Valid Accuracy: 81.67%\n",
      "\n",
      "Epoch 4/10\n",
      "Loss: 0.1596\n",
      "Train Accuracy: 95.39%\n",
      "Valid Accuracy: 85.00%\n",
      "\n",
      "Epoch 5/10\n",
      "Loss: 0.1679\n",
      "Train Accuracy: 94.63%\n",
      "Valid Accuracy: 90.00%\n",
      "\n",
      "Epoch 6/10\n",
      "Loss: 0.1768\n",
      "Train Accuracy: 93.87%\n",
      "Valid Accuracy: 93.33%\n",
      "\n",
      "Epoch 7/10\n",
      "Loss: 0.0990\n",
      "Train Accuracy: 96.67%\n",
      "Valid Accuracy: 91.67%\n",
      "\n",
      "Epoch 8/10\n",
      "Loss: 0.0872\n",
      "Train Accuracy: 97.43%\n",
      "Valid Accuracy: 95.00%\n",
      "\n",
      "Epoch 9/10\n",
      "Loss: 0.1066\n",
      "Train Accuracy: 97.28%\n",
      "Valid Accuracy: 90.00%\n",
      "\n",
      "Epoch 10/10\n",
      "Loss: 0.1085\n",
      "Train Accuracy: 96.14%\n",
      "Valid Accuracy: 93.33%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training components\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Changed to Adam\n",
    "\n",
    "# Training loop\n",
    "print(\"Starting training...\")\n",
    "num_epochs = 10  # epochs increased\n",
    "best_val_acc = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    # Calculate metrics\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = correct / total\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    valid_correct = 0\n",
    "    valid_total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in valid_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            valid_correct += (predicted == labels).sum().item()\n",
    "            valid_total += labels.size(0)\n",
    "\n",
    "    valid_acc = valid_correct / valid_total\n",
    "\n",
    "    # Save best model\n",
    "    if valid_acc > best_val_acc:\n",
    "        best_val_acc = valid_acc\n",
    "        torch.save(model.state_dict(), 'best_plant_disease_model.pth')\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Loss: {epoch_loss:.4f}\")\n",
    "    print(f\"Train Accuracy: {epoch_acc*100:.2f}%\")\n",
    "    print(f\"Valid Accuracy: {valid_acc*100:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_24040\\2132295733.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_plant_disease_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 92.00%\n"
     ]
    }
   ],
   "source": [
    "# Load the best model\n",
    "model.load_state_dict(torch.load('best_plant_disease_model.pth'))\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, test_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    print(f\"Test Accuracy: {accuracy*100:.2f}%\")\n",
    "\n",
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
